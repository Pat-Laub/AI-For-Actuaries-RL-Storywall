# ACTL3143 Storywall

Run `pip install -r requirements.txt` so it installs the packages you need to run the code. I would recommend doing this in a [virtual environment](https://docs.python.org/3/library/venv.html) as some of the packages have recently been updated which could cause some annoying version conflicts with any other packages you may have. Additionally, this set of requirements was created using Python 3.11.6, I haven't tested it fully with Python 3.12, but from what I have tried it seemed to work.

In order to render the videos so you can see how your agent plays, you'll need to install [FFmpeg](https://ffmpeg.org/).

If you want to run the training on your GPU, you'll need to install CUDA using the NVIDIA CUDA installation guide for your system. Some things to note
- Before picking which version to install make sure it works with your GPU, [this post](https://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with) was helpful for that
- You also need to install the required packages for PyTorch to work with CUDA [here](https://pytorch.org/get-started/locally/). Note that PyTorch only works with some versions of CUDA (these are listed on that website as options) so ensure that whichever version you install works with PyTorch. 
- Make sure to also follow any required pip installs for using CUDA with Python (something like nvidia_cuda_runtime_cu12, should be listed in installation guide)


# The Folders and Running Code

You can adjust various parameters including the algorithm and the game in [`setup.py`](setup.py). The functions in [`helper.py`](helper.py) probably don't need to be used, if you're interested have a look but they'll work as is.

To actually run the training algorithm you just need to run [`training.py`](training.py). 

Doing this will create a folder `Results/{GAME_NAME}/{ALGORITHM_NAME}/V_` with the new version number being worked out by the program.

In here there is
- `evals` which stores
    - `best_model.zip` - the best model iteration so far
    - `evaluation_reward_logs.npz` - contains the reward logs generated by `evaluation.py` (use `np.load` to load the object into Python)
    - `evaluations.npz` - contains mean reward and episode length data generated by evaluations through the learning process (the ones triggered by `EvalCallback` in `training.py`)
    - `evaluations.png` - a graph of the above data
    - `learning_reward_logs.npz` - contains reward logs generated by evaluations through the learning process
- `saves` - the model saved at various specific step counts. You can change model used for evaluation in `evaluation.py` if you want to assess the model at different stages of training
- `videos` - this will appear after you use `evaluation.py` and stores the recordings of the evaluation runs.

At any point (including while it's training), you can run [`evaluation.py`](evaluation.py) with a command line argument equivalent to whichever version you would like to evaluate.

For example if I trained one version and want to assess it I'd run `python evaluation.py V1`.

Other than that it should be all good, enjoy \:))

